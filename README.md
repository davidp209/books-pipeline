# Books Pipeline: Ingesta y UnificaciÃ³n de Datos de Libros (Proyecto RA1)

Este proyecto implementa un pipeline ETL (Extract, Transform, Load) diseÃ±ado para construir un catÃ¡logo de libros robusto y normalizado (`dim_book`). El sistema extrae informaciÃ³n social y descriptiva de **Goodreads**, la enriquece con datos comerciales (precios, ISBNs precisos) de **Google Books API**, y unifica ambas fuentes mediante un algoritmo de resoluciÃ³n de entidades.

## ğŸ“‹ DescripciÃ³n del Flujo

El pipeline opera en tres etapas secuenciales:

1. **Scraping (Goodreads):** ObtenciÃ³n de metadatos, ratings y descripciones mediante scraping hÃ­brido (HTML parsing + JSON-LD).
2. **Enriquecimiento (Google Books):** BÃºsqueda de libros coincidentes vÃ­a API para completar ISBNs faltantes y precios.
3. **NormalizaciÃ³n y Merge:** FusiÃ³n de ambas fuentes priorizando la calidad del dato, limpieza de strings y generaciÃ³n de una tabla dimensional final.

## ğŸ“‚ Estructura

```bash
BOOKS-PIPELINE/
â”‚
â”œâ”€â”€ ğŸ“‚ docs/
â”‚   â”œâ”€â”€ quality_metrics.json â†’ mÃ©tricas de calidad (filas, cruces, totales)
â”‚   â””â”€â”€ schema.md â†’ esquema documentado de la tabla final
â”‚
â”œâ”€â”€ ğŸ“‚ landing/
â”‚   â”œâ”€â”€ goodreads_books.json â†’ fuente bruta de Goodreads (JSONL)
â”‚   â””â”€â”€ googlebooks_books.csv â†’ datos enriquecidos desde Google Books
â”‚
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ scraper_goodreads.py  â†’ extracciÃ³n (scraping) desde Goodreads
â”‚   â”œâ”€â”€ enrich_googlebooks.py â†’ enriquecimiento usando Google Books 
â”‚   â””â”€â”€ integrate_pipeline.py â†’  integraciÃ³n / merge / normalizaciÃ³n
â”‚
â”œâ”€â”€ ğŸ“‚ standard/
â”‚   â”œâ”€â”€ dim_book.parquet â†’ tabla maestra de libros (modelo canÃ³nico)
â”‚   â””â”€â”€ book_source_detail.parquet â†’ detalle de trazabilidad de fuentes
â”‚
â””â”€â”€ requirements.txt â†’ dependencias del proyecto

```

## ğŸš€ CÃ³mo Ejecutar

- Python 3.8+ (especificar versiÃ³n exacta si aplica)

### 1ï¸âƒ£ Instalar dependencias:

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Activar entorno virtual

```bash
venv\Scripts\activate
```

## ğŸš€ EjecuciÃ³n paso a paso

### 1ï¸âƒ£ Scrapear datos de Goodreads

Obtiene metadatos, descripciones y ratings mediante scraping.

```bash
python src/scraper_goodreads.py
```

### 2ï¸âƒ£ Enriquecer datos usando Google Books API

Busca precios e ISBNs faltantes mediante coincidencia difusa (fuzzy matching).

```bash
python src/enrich_googlebooks.py
```

### 3ï¸âƒ£ Integrar y normalizar en el modelo canÃ³nico

Ejecuta la lÃ³gica de fusiÃ³n ("Survivor Value"), limpieza y deduplicaciÃ³n.

```bash
python src/integrate_pipeline.py
```

## ğŸ—‚ Metadatos y configuraciones

* **Separador CSV:** `;`
* **CodificaciÃ³n:** UTF-8
* **Selectores/UA:** No aplica, se usan archivos locales; ingestiÃ³n proviene de `ingestion_date` de cada fuente.
* **NormalizaciÃ³n de datos:**

  * Autores y categorÃ­as â†’ unidos con `|`.
  * TÃ­tulos â†’ normalizados sin puntuaciÃ³n ni espacios extra (`title_normalized`).
  * Precios â†’ normalizados a ISO 4217 (`USD`, `EUR`, `GBP`).

## ğŸ”‘ Decisiones clave

* **UnificaciÃ³n de fuentes**

  * Prioridad basada en **score de completitud** entre Goodreads y Google Books.
  * `source_preference` indica la fuente elegida.
* **Canonical ID**

  * Si existe ISBN13 â†’ se usa como PK.
  * Si no â†’ SHA1 de `title_normalized + first_author + publisher + pub_year`.
* **Matching heurÃ­stico**

  * Primero por ID explÃ­cito (`id` de Goodreads vs `gb_id` de Google Books).
  * Luego por ISBN13.
  * Finalmente por combinaciÃ³n `title_normalized + first_author`.
* **Manejo de tipos mixtos**

  * Strings y nÃºmeros normalizados para evitar errores Parquet (`ArrowTypeError`).
  * Fechas convertidas a ISO 8601; nulos permitidos.
* **Fallback de guardado**

  * Si Parquet falla, se guarda en CSV con conversiÃ³n a string para evitar pÃ©rdida de datos.

  ## ğŸ—ƒ Esquema de `dim_book

[schema.md](./docs/schema.md)

## ğŸ“ Notas adicionales

* NormalizaciÃ³n garantiza consistencia para matching y anÃ¡lisis.
* Canonical ID asegura unicidad y estabilidad.
* Merge prioriza fuente con mayor completitud.
* MÃ©tricas de calidad (`quality_metrics.json`) ayudan a auditar la cobertura y coincidencia entre fuentes.
* Fechas `pub_date` y `ingestion_date_*` en formato UTC ISO 8601.

# ğŸ›¡ Idempotencia y DeduplicaciÃ³n en merge_books_pipeline

## ğŸ”¹ Idempotencia

El pipeline estÃ¡ diseÃ±ado para **ser idempotente**, es decir, **puede ejecutarse varias veces sin crear duplicados ni inconsistencias** en la tabla `dim_book`.

**Mecanismos que garantizan la idempotencia:**

1. **Canonical ID Ãºnico**

   - Se genera un `canonical_id` para cada libro:
     - Si existe `isbn13` â†’ se usa como ID.
     - Si no â†’ SHA1 de `[title_normalized, first_author, publisher, pub_year]`.
   - Esto asegura que el mismo libro tenga siempre el mismo identificador, sin importar cuÃ¡ntas veces se ejecute el pipeline.
2. **NormalizaciÃ³n consistente**

   - TÃ­tulos, autores, categorÃ­as y fechas se normalizan antes del merge:
     - `title_normalized`: minÃºsculas, sin puntuaciÃ³n ni espacios extra.
     - `authors`: lista Ãºnica separada por `|`.
     - `categories`: lista Ãºnica separada por `|`.
   - Evita que diferencias de formato generen duplicados.
3. **Merge determinista**

   - Matching en tres niveles:

     1. `gb_id` (ID de Google Books)
     2. `isbn13`
     3. Clave heurÃ­stica `title_normalized + first_author`
4. **Salida reproducible**

   - Los archivos Parquet/CSV generados contienen siempre las mismas filas si los datos de entrada no cambian.
   - Las mÃ©tricas de calidad (`quality_metrics.json`) reflejan los resultados de forma consistente.

---

## ğŸ”¹ DeduplicaciÃ³n

El pipeline incluye un paso explÃ­cito de **deduplicaciÃ³n** para eliminar registros repetidos en `dim_book`.

**CÃ³mo se implementa:**

1. Se aÃ±ade una columna temporal `_score`:
   ```python
   df_final["_score"] = df_final.notnull().sum(axis=1)
   ```
